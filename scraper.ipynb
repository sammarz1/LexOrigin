{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acafe9cf-c2fb-43e3-9bae-ee8613323657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_posts(subreddit, limit=75000):\n",
    "    headers = {\"User-Agent\": \"custom-script\"}\n",
    "    url = f\"https://www.reddit.com/r/{subreddit}/top/.json?t=week&limit=100\"\n",
    "    posts = []\n",
    "    after = None\n",
    "\n",
    "    while len(posts) < limit:\n",
    "        full_url = url + (f\"&after={after}\" if after else \"\")\n",
    "        response = requests.get(full_url, headers=headers)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Error: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "        data = response.json()\n",
    "        children = data[\"data\"][\"children\"]\n",
    "        \n",
    "        if not children:\n",
    "            break\n",
    "\n",
    "        for post in children:\n",
    "            post_data = post[\"data\"]\n",
    "            posts.append(post_data)\n",
    "            if len(posts) >= limit:\n",
    "                break\n",
    "\n",
    "        after = data[\"data\"].get(\"after\")\n",
    "        if not after:\n",
    "            print(\"‚úÖ No more posts to fetch.\")\n",
    "            break\n",
    "\n",
    "        time.sleep(1)  # Avoid rate limits\n",
    "    \n",
    "    print(f\"‚úÖ Fetched {len(posts)} posts.\")\n",
    "    return posts\n",
    "\n",
    "def fetch_comments(post_permalink, headers):\n",
    "    comments_url = f\"https://www.reddit.com{post_permalink}.json?limit=500\"\n",
    "    response = requests.get(comments_url, headers=headers)\n",
    "    comments = []\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        comments_data = response.json()\n",
    "        \n",
    "        if len(comments_data) > 1:\n",
    "            for comment in comments_data[1][\"data\"][\"children\"]:\n",
    "                if \"body\" in comment[\"data\"]:\n",
    "                    comments.append(comment[\"data\"][\"body\"])\n",
    "    \n",
    "    return comments\n",
    "\n",
    "def scrape_reddit(subreddit, post_limit=75000):\n",
    "    headers = {\"User-Agent\": \"custom-script\"}\n",
    "    all_text = []\n",
    "    posts = fetch_posts(subreddit, post_limit)\n",
    "\n",
    "    for idx, post in enumerate(posts):\n",
    "        post_body = post.get(\"selftext\", \"[No text]\")\n",
    "        all_text.append(post_body)\n",
    "        \n",
    "        comments = fetch_comments(post[\"permalink\"], headers)\n",
    "        all_text.extend(comments)\n",
    "        \n",
    "        if idx % 100== 0:\n",
    "            print(f\"üì• Processed {idx}/{len(posts)} posts...\")\n",
    "\n",
    "        time.sleep(1)  # Avoid rate limits\n",
    "\n",
    "    print(f\"‚úÖ Fetched total {len(all_text)} posts & comments.\")\n",
    "    return all_text\n",
    "\n",
    "# Run the scraper\n",
    "subreddit = \"AskSpain\"\n",
    "data = scrape_reddit(subreddit, post_limit=100)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'TEXTO':data})\n",
    "df['PA√çS'] = ['Spain']*len(data)\n",
    "df['FUENTE'] = ['r/AskSpain']*len(data)\n",
    "df.to_csv('AskSpain2.csv')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
